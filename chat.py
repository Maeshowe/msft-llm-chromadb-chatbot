import yaml
import logging
import os
import time
from langchain_community.llms import LlamaCpp
from langchain_huggingface.embeddings import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

with open('config.yaml', 'r', encoding='utf-8') as f:
    config = yaml.safe_load(f)

# Loggol√°s be√°ll√≠t√°sa
logs_dir = config['paths']['logs_dir']
os.makedirs(logs_dir, exist_ok=True)
logging.basicConfig(
    filename=os.path.join(logs_dir, 'chat.log'),
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    encoding='utf-8'
)

model_conf = config['llm']['models'][config['llm']['active_model']]
vectorstore_path = config['paths']['vectorstore_dir']

llm = LlamaCpp(
    model_path=model_conf["model_path"],
    temperature=model_conf["temperature"],
    max_tokens=model_conf["max_tokens"],
    n_ctx=model_conf["context_size"],
    n_batch=model_conf["n_batch"],
    n_threads=model_conf["n_threads"],
    verbose=config["llm"].get("verbose", False),
    n_gpu_layers=0
)

embeddings = HuggingFaceEmbeddings(model_name=config["embedding"]["model"])
db = Chroma(persist_directory=vectorstore_path, embedding_function=embeddings)
retriever = db.as_retriever(search_kwargs={"k": config["similarity_search"]["top_k"]})

prompt = PromptTemplate(
    input_variables=["context", "question"],
    template=config['prompt']['template']
)

qa_chain = RetrievalQA.from_chain_type(
    llm,
    chain_type="stuff",
    retriever=retriever,
    return_source_documents=True,
    chain_type_kwargs={"prompt": prompt}
)

print("üöÄ Chatbot elindult (kil√©p√©s: 'exit')")

while True:
    query = input("\nüëâ K√©rd√©sed: ")
    if query.lower() in ['exit', 'quit']:
        break

    start_time = time.time()  # ‚úÖ hi√°nyz√≥ defin√≠ci√≥ hozz√°adva
    result = qa_chain.invoke(query)
    inference_time = time.time() - start_time

    response = result['result']
    sources = [doc.metadata.get('source', 'n/a') for doc in result['source_documents']]

    if not sources:
        logging.warning(f"Nincs tal√°lat: {query}")

    logging.info(f"K√©rd√©s: {query}")
    logging.info(f"V√°lasz: {response}")
    logging.info(f"Forr√°sok: {sources}")
    logging.info(f"Inferencia id≈ë: {inference_time:.2f}s")

    print(f"\nü§ñ V√°lasz: {response}")
    print(f"üìö Forr√°s(ok): {', '.join(set(sources))}")
